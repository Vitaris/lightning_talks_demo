{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d824f8d0",
   "metadata": {},
   "source": [
    "# ü•´ YOLO Can Detection ‚Äî Lightning Talk Demo\n",
    "\n",
    "This notebook walks through the full pipeline:\n",
    "\n",
    "1. **Data Preparation** ‚Äî explore raw data, split into train/val, visualize annotations\n",
    "2. **Training** ‚Äî fine-tune YOLOv8n on our can dataset\n",
    "3. **Demo Monitor** ‚Äî live camera inference with Basler GigE camera\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad153da0",
   "metadata": {},
   "source": [
    "## 0. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17971166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:      3.12.3\n",
      "PyTorch:     2.6.0+cu124\n",
      "CUDA:        True\n",
      "GPU:         NVIDIA GeForce RTX 3060\n",
      "VRAM:        11.6 GB\n",
      "Ultralytics: 8.4.16\n",
      "OpenCV:      4.13.0\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, cv2\n",
    "import ultralytics\n",
    "\n",
    "print(f\"Python:      {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch:     {torch.__version__}\")\n",
    "print(f\"CUDA:        {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:         {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM:        {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "print(f\"OpenCV:      {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb4803",
   "metadata": {},
   "source": [
    "### 1.4 Split Dataset (Train / Val)\n",
    "\n",
    "Split the raw `cans/` data into `dataset/images/{train,val}` and `dataset/labels/{train,val}` with an 80/20 ratio.\n",
    "\n",
    "This is what `prepare_dataset.py` does ‚Äî we run it inline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7f0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old dataset/\n",
      "Train: 312 images\n",
      "Val:   79 images\n",
      "\n",
      "Dataset structure:\n",
      "  dataset/images/  (2 files)\n",
      "  dataset/images/train/  (312 files)\n",
      "  dataset/images/val/  (79 files)\n",
      "  dataset/labels/  (2 files)\n",
      "  dataset/labels/train/  (312 files)\n",
      "  dataset/labels/val/  (79 files)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "# Clean old dataset\n",
    "if Path(\"dataset\").exists():\n",
    "    shutil.rmtree(\"dataset\")\n",
    "    print(\"Removed old dataset/\")\n",
    "\n",
    "# Create directory structure\n",
    "for split in [\"train\", \"val\"]:\n",
    "    Path(f\"dataset/images/{split}\").mkdir(parents=True)\n",
    "    Path(f\"dataset/labels/{split}\").mkdir(parents=True)\n",
    "\n",
    "# Shuffle and split\n",
    "all_images = sorted(Path(\"cans\").glob(\"*.jpg\"))\n",
    "random.seed(42)  # reproducible split\n",
    "random.shuffle(all_images)\n",
    "\n",
    "split_idx = int(0.8 * len(all_images))\n",
    "splits = {\n",
    "    \"train\": all_images[:split_idx],\n",
    "    \"val\":   all_images[split_idx:],\n",
    "}\n",
    "\n",
    "for split_name, file_list in splits.items():\n",
    "    for img_file in file_list:\n",
    "        lbl_file = img_file.with_suffix(\".txt\")\n",
    "        shutil.copy2(img_file, f\"dataset/images/{split_name}/{img_file.name}\")\n",
    "        if lbl_file.exists():\n",
    "            shutil.copy2(lbl_file, f\"dataset/labels/{split_name}/{lbl_file.name}\")\n",
    "\n",
    "print(f\"Train: {len(splits['train'])} images\")\n",
    "print(f\"Val:   {len(splits['val'])} images\")\n",
    "print(\"\\nDataset structure:\")\n",
    "for p in sorted(Path(\"dataset\").rglob(\"*\")):\n",
    "    if p.is_dir():\n",
    "        count = len(list(p.glob(\"*\")))\n",
    "        print(f\"  {p}/  ({count} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e0eba",
   "metadata": {},
   "source": [
    "### 1.5 Verify `cans.yaml`\n",
    "\n",
    "YOLO needs a dataset config file pointing to our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cd59a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ./dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n",
      "nc: 3  # Adjust based on your classes\n",
      "names: ['0_5L', '0_33L', '0_25L']  # Update with your actual class names\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Path(\"cans.yaml\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0604d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training\n",
    "\n",
    "Train **YOLOv8 Nano** from scratch on our can dataset.\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Base model | `yolov8n.yaml` (random weights, no pretraining) |\n",
    "| Image size | 640 |\n",
    "| Epochs | 10 |\n",
    "| Dataset | `cans.yaml` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77626f5",
   "metadata": {},
   "source": [
    "### 2.1 Train from Scratch\n",
    "\n",
    "Train a YOLOv8 Nano model from random weights (no pretrained COCO weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366dd2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.16 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 11909MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=cans.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/home/viktor/lightning_talks_demo/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, 16, None, [64, 128, 256]] \n",
      "YOLOv8n summary: 130 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5556.8¬±3202.1 MB/s, size: 1661.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/viktor/lightning_talks_demo/dataset/labels/train.cache... 312 images, 7 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 312/312 100.7Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/train/IMG_20260221_232458_2_result.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/train/IMG_20260221_232458_3_result.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/train/IMG_20260221_232458_5_result.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2341.3¬±731.7 MB/s, size: 1662.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/viktor/lightning_talks_demo/dataset/labels/val.cache... 79 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79/79 8.1Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/val/IMG_20260222_082515_result.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/val/IMG_20260222_085857_result.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/viktor/lightning_talks_demo/dataset/images/val/IMG_20260222_085904_result.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Plotting labels to /home/viktor/lightning_talks_demo/runs/detect/train3/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/viktor/lightning_talks_demo/runs/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      2.03G      3.464       4.38      4.291         49        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 1.5it/s 13.4s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 3.2it/s 0.9s0.7s\n",
      "                   all         79        602   6.72e-05    0.00267    3.4e-05    3.4e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      2.14G      2.911      3.899      4.045         53        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.0it/s 2.9s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 4.0it/s 0.8s0.6s\n",
      "                   all         79        602   6.65e-05    0.00267   3.37e-05   3.37e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      2.15G      2.448      3.262      3.612         55        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.0it/s 2.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 7.3it/s 0.4s0.3s\n",
      "                   all         79        602   6.99e-05    0.00267   3.56e-05   3.56e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      2.15G      2.071      2.703       3.13         57        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 6.8it/s 2.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 5.4it/s 0.6s0.4s\n",
      "                   all         79        602   7.25e-05    0.00267   3.71e-05   3.71e-06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      2.15G       1.87      2.383      2.598         52        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 6.9it/s 2.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.0it/s 0.5s0.4s\n",
      "                   all         79        602   0.000136    0.00527   7.17e-05   1.36e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      2.16G      1.743      2.211      2.324         88        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 6.5it/s 3.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.3it/s 1.3s1.1s\n",
      "                   all         79        602       0.14     0.0171     0.0228    0.00609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      2.18G      1.601       2.05      2.111         47        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.2it/s 2.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 4.4it/s 0.7s0.5s\n",
      "                   all         79        602      0.297      0.426      0.268      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      2.18G       1.52      1.873      1.968         53        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.3it/s 2.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 3.6it/s 0.8s0.6s\n",
      "                   all         79        602      0.392      0.569      0.386      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      2.18G      1.436      1.773      1.891         54        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.4it/s 2.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 5.2it/s 0.6s0.4s\n",
      "                   all         79        602      0.416      0.647      0.465      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      2.18G      1.387      1.746      1.818         88        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 7.3it/s 2.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 4.7it/s 0.6s0.5s\n",
      "                   all         79        602      0.473      0.655      0.531      0.357\n",
      "\n",
      "10 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from /home/viktor/lightning_talks_demo/runs/detect/train3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/viktor/lightning_talks_demo/runs/detect/train3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/viktor/lightning_talks_demo/runs/detect/train3/weights/best.pt...\n",
      "Ultralytics 8.4.16 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 11909MiB)\n",
      "YOLOv8n summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 3.5it/s 0.9s0.5s\n",
      "                   all         79        602      0.474      0.658      0.531      0.357\n",
      "                  0_5L         46        349      0.739      0.817       0.85      0.566\n",
      "                 0_33L         32        125      0.431      0.712      0.485      0.339\n",
      "                 0_25L         32        128      0.253      0.445      0.257      0.165\n",
      "Speed: 0.2ms preprocess, 1.9ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/viktor/lightning_talks_demo/runs/detect/train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Build YOLOv8 Nano from scratch (random weights, no pretraining)\n",
    "model = YOLO(\"yolov8n.yaml\")\n",
    "\n",
    "# Train on our dataset\n",
    "results = model.train(data=\"cans.yaml\", epochs=10, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a7506",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Demo Monitor ‚Äî Live Camera Inference\n",
    "\n",
    "The `demo_monitor.py` script runs a live loop:\n",
    "\n",
    "```\n",
    "Basler GigE Camera ‚Üí YOLO Inference (GPU) ‚Üí OpenCV Display + Side Panel\n",
    "```\n",
    "\n",
    "**Controls:**\n",
    "| Key | Action |\n",
    "|-----|--------|\n",
    "| `q` | Quit |\n",
    "| `s` | Save screenshot |\n",
    "| `r` | Start/stop recording |\n",
    "| `c` | Clear FPS stats |\n",
    "\n",
    "> ‚ö†Ô∏è Requires a connected **Basler GigE camera** and the `pypylon` package.  \n",
    "> The cell below launches the monitor as a subprocess so the notebook stays responsive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c0b3a",
   "metadata": {},
   "source": [
    "### 3.1 Install pypylon (camera SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee0ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypylon\n",
      "  Downloading pypylon-26.1.0-cp39-abi3-manylinux_2_31_x86_64.whl.metadata (9.2 kB)\n",
      "Downloading pypylon-26.1.0-cp39-abi3-manylinux_2_31_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypylon\n",
      "Successfully installed pypylon-26.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypylon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413073c5",
   "metadata": {},
   "source": [
    "### 3.2 Launch Demo Monitor\n",
    "\n",
    "This opens an OpenCV window with live detections.  \n",
    "Press **q** in the OpenCV window to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7148fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can Detector Monitor\n",
      "========================================\n",
      "Loading model: best_model/best.pt\n",
      "Model loaded (GPU)\n",
      "Camera: acA1300-60gm\n",
      "Camera ready\n",
      "Controls: 'q' quit | 's' screenshot | 'r' record | 'c' clear stats\n",
      "QFontDatabase: Cannot find font directory /home/viktor/lightning_talks_demo/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/viktor/lightning_talks_demo/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/viktor/lightning_talks_demo/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/viktor/lightning_talks_demo/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "QFontDatabase: Cannot find font directory /home/viktor/lightning_talks_demo/venv/lib/python3.12/site-packages/cv2/qt/fonts.\n",
      "Note that Qt no longer ships fonts. Deploy some (from https://dejavu-fonts.github.io/ for example) or switch to fontconfig.\n",
      "^C\n",
      "\n",
      "Stopped by user\n",
      "Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "!python demo_monitor.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
